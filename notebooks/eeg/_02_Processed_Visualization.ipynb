{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0a6d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading aggregated data from: /Users/patriciawatanabe/Projects/Neurotech/NTUT25_Software/data/eeg/processed/aggregated/SINGLETRIAL_REST_AGG_processed.pkl\n",
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Set up Project Root and Paths ---\n",
    "# Assuming this notebook is in 'notebooks/', we go up one level to 'NTUT25_SOFTWARE'\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..', '..')) \n",
    "\n",
    "# Add 'src' directory to Python path to import config\n",
    "import sys\n",
    "sys.path.append(os.path.join(PROJECT_ROOT))\n",
    "\n",
    "from src.eeg import config\n",
    "\n",
    "# --- Define Paths ---\n",
    "PROCESSED_EEG_DIR = os.path.join(config.PROCESSED_EEG_DATA_DIR, 'aggregated')\n",
    "AGGREGATED_FNAME = os.path.join(PROCESSED_EEG_DIR, config.AGGREGATED_FNAME)\n",
    "\n",
    "# --- Load the aggregated data ---\n",
    "print(f\"Loading aggregated data from: {AGGREGATED_FNAME}\")\n",
    "\n",
    "try:\n",
    "    with open(AGGREGATED_FNAME, 'rb') as f:\n",
    "        matched_data = pickle.load(f)\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Aggregated data file not found at {AGGREGATED_FNAME}.\")\n",
    "    matched_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0a6d92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded reference Info from /Users/patriciawatanabe/Projects/Neurotech/NTUT25_Software/data/eeg/processed/804_1_PD_REST_processed-epo.fif.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Info (63) and data (11) must have same number of channels.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m ctl_closed_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([d[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m matched_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEyes_Closed\u001b[39m\u001b[38;5;124m'\u001b[39m]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Create Evoked objects (which are just averages) with the NEW Info object\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m pd_on_open_evoked \u001b[38;5;241m=\u001b[39m \u001b[43mmne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEvokedArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd_on_open_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo_100hz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m pd_off_open_evoked \u001b[38;5;241m=\u001b[39m mne\u001b[38;5;241m.\u001b[39mEvokedArray(np\u001b[38;5;241m.\u001b[39mmean(pd_off_open_data, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), info_100hz, tmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     40\u001b[0m ctl_open_evoked \u001b[38;5;241m=\u001b[39m mne\u001b[38;5;241m.\u001b[39mEvokedArray(np\u001b[38;5;241m.\u001b[39mmean(ctl_open_data, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), info_100hz, tmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m<decorator-gen-256>:12\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, data, info, tmin, comment, nave, kind, baseline, verbose)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pd_tremor/lib/python3.9/site-packages/mne/evoked.py:929\u001b[0m, in \u001b[0;36mEvokedArray.__init__\u001b[0;34m(self, data, info, tmin, comment, nave, kind, baseline, verbose)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData must be a 2D array of shape (n_channels, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    926\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_samples), got shape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (data\u001b[38;5;241m.\u001b[39mshape,))\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mch_names\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mshape(data)[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 929\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInfo (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) and data (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) must have same number \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    930\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mof channels.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mch_names\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m    931\u001b[0m                                        np\u001b[38;5;241m.\u001b[39mshape(data)[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(tmin \u001b[38;5;241m*\u001b[39m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msfreq\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[0;31mValueError\u001b[0m: Info (63) and data (11) must have same number of channels."
     ]
    }
   ],
   "source": [
    "# To use MNE's plotting tools, we need to convert our NumPy arrays back to MNE objects.\n",
    "\n",
    "# --- Load a reference Info object from a processed file ---\n",
    "# Pick one of the processed .fif files created in the previous step\n",
    "REF_FNAME_BASE = \"804_1_PD_REST_processed\"\n",
    "REF_PROCESSED_FNAME = os.path.join(config.PROCESSED_EEG_DATA_DIR, f\"{REF_FNAME_BASE}{config.RAW_FNAME_SUFFIX}\")\n",
    "\n",
    "try:\n",
    "    epochs_ref = mne.read_epochs(REF_PROCESSED_FNAME, preload=False, verbose=False)\n",
    "    info_ref = epochs_ref.info\n",
    "    print(f\"Loaded reference Info from {REF_PROCESSED_FNAME}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Reference file not found at {REF_PROCESSED_FNAME}. Cannot reconstruct MNE objects.\")\n",
    "    info_ref = None\n",
    "\n",
    "# --- Reconstruct Evoked Objects for each condition and group ---\n",
    "if matched_data and info_ref:\n",
    "    # Get the channel names from the reference Info object\n",
    "    ch_names_ref = info_ref['ch_names']\n",
    "    \n",
    "    # Create a NEW Info object with the correct final sampling frequency (100 Hz)\n",
    "    info_100hz = mne.create_info(ch_names=ch_names_ref, sfreq=config.FINAL_SAMPLING_RATE, ch_types='eeg') # Assuming all final channels are EEG\n",
    "    \n",
    "    # Apply the montage to this new Info object\n",
    "    montage = mne.channels.make_standard_montage('standard_1005') \n",
    "    info_100hz.set_montage(montage, on_missing='ignore') \n",
    "\n",
    "    # Concatenate data across subjects for each group\n",
    "    pd_on_open_data = np.concatenate([d[0] for d in matched_data['Eyes_Open']], axis=0)\n",
    "    pd_off_open_data = np.concatenate([d[1] for d in matched_data['Eyes_Open']], axis=0)\n",
    "    ctl_open_data = np.concatenate([d[2] for d in matched_data['Eyes_Open']], axis=0)\n",
    "\n",
    "    pd_on_closed_data = np.concatenate([d[0] for d in matched_data['Eyes_Closed']], axis=0)\n",
    "    pd_off_closed_data = np.concatenate([d[1] for d in matched_data['Eyes_Closed']], axis=0)\n",
    "    ctl_closed_data = np.concatenate([d[2] for d in matched_data['Eyes_Closed']], axis=0)\n",
    "    \n",
    "    # Create Evoked objects (which are just averages) with the NEW Info object\n",
    "    pd_on_open_evoked = mne.EvokedArray(np.mean(pd_on_open_data, axis=0), info_100hz, tmin=0)\n",
    "    pd_off_open_evoked = mne.EvokedArray(np.mean(pd_off_open_data, axis=0), info_100hz, tmin=0)\n",
    "    ctl_open_evoked = mne.EvokedArray(np.mean(ctl_open_data, axis=0), info_100hz, tmin=0)\n",
    "    \n",
    "    pd_on_closed_evoked = mne.EvokedArray(np.mean(pd_on_closed_data, axis=0), info_100hz, tmin=0)\n",
    "    pd_off_closed_evoked = mne.EvokedArray(np.mean(pd_off_closed_data, axis=0), info_100hz, tmin=0)\n",
    "    ctl_closed_evoked = mne.EvokedArray(np.mean(ctl_closed_data, axis=0), info_100hz, tmin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd7584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNE provides a convenient way to visualize electrode locations on a head model\n",
    "if info_ref:\n",
    "    fig = mne.viz.plot_sensors(info_ref, kind='head')\n",
    "    fig.set_size_inches(6, 6)\n",
    "    plt.show()\n",
    "\n",
    "    # To see the names more clearly\n",
    "    fig = mne.viz.plot_sensors(info_ref, kind='topomap', show_names=True, ch_type='eeg')\n",
    "    fig.set_size_inches(8, 8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d1f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pd_on_open_evoked:\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6), sharey=True)\n",
    "    \n",
    "    # Plot Eyes-Open ERPs for all groups\n",
    "    mne.viz.plot_compare_evokeds(\n",
    "        dict(PD_ON=pd_on_open_evoked, PD_OFF=pd_off_open_evoked, CTL=ctl_open_evoked),\n",
    "        picks=['Fz', 'Cz', 'Pz'], # Pick some key EEG channels\n",
    "        title='Eyes-Open ERP',\n",
    "        axes=axes[0],\n",
    "        show=False\n",
    "    )\n",
    "    axes[0].legend(loc='upper right')\n",
    "    \n",
    "    # Plot Eyes-Closed ERPs for all groups\n",
    "    mne.viz.plot_compare_evokeds(\n",
    "        dict(PD_ON=pd_on_closed_evoked, PD_OFF=pd_off_closed_evoked, CTL=ctl_closed_evoked),\n",
    "        picks=['Fz', 'Cz', 'Pz'],\n",
    "        title='Eyes-Closed ERP',\n",
    "        axes=axes[1],\n",
    "        show=False\n",
    "    )\n",
    "    axes[1].legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pd_on_open_evoked:\n",
    "    # A time window for a potential alpha rhythm peak (e.g., 0.5-2 seconds)\n",
    "    alpha_window = (0.5, 2.0)\n",
    "\n",
    "    # Plot topomap for Eyes-Closed (where alpha is prominent)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "    \n",
    "    mne.viz.plot_topomap(pd_on_closed_evoked.data[:, pd_on_closed_evoked.time_as_index(alpha_window)[0]],\n",
    "                         pd_on_closed_evoked.info,\n",
    "                         axes=axes[0],\n",
    "                         show=False,\n",
    "                         vlim=(-4, 4), # Set consistent color limits for comparison\n",
    "                         title='PD ON Closed')\n",
    "\n",
    "    mne.viz.plot_topomap(pd_off_closed_evoked.data[:, pd_off_closed_evoked.time_as_index(alpha_window)[0]],\n",
    "                         pd_off_closed_evoked.info,\n",
    "                         axes=axes[1],\n",
    "                         show=False,\n",
    "                         vlim=(-4, 4),\n",
    "                         title='PD OFF Closed')\n",
    "                         \n",
    "    mne.viz.plot_topomap(ctl_closed_evoked.data[:, ctl_closed_evoked.time_as_index(alpha_window)[0]],\n",
    "                         ctl_closed_evoked.info,\n",
    "                         axes=axes[2],\n",
    "                         show=False,\n",
    "                         vlim=(-4, 4),\n",
    "                         title='CTL Closed')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf57f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SOP doesn't mention TF for REST, but it was in your Oddball analysis.\n",
    "# Let's show how you would do it on the epochs.\n",
    "\n",
    "if matched_data and info_ref:\n",
    "    # We need to take the raw epoched data for a group, not the average.\n",
    "    epochs_ctl_open = matched_data['Eyes_Open'][0][2] # First pair, CTL data\n",
    "    \n",
    "    # The data in matched_data is already a NumPy array.\n",
    "    # We need to reconstruct an MNE Epochs object to run TF on it.\n",
    "    epochs_ctl_open_mne = mne.EpochsArray(epochs_ctl_open, info_ref_100hz, tmin=0, verbose=False)\n",
    "    \n",
    "    # Frequencies to analyze (e.g., 4 to 30 Hz for Alpha, Beta bands)\n",
    "    freqs = np.arange(4, 30, 1)\n",
    "    \n",
    "    # Compute Morlet wavelet time-frequency representation\n",
    "    power = mne.time_frequency.tfr_morlet(epochs_ctl_open_mne, freqs=freqs, \n",
    "                                          n_cycles=freqs/2, return_itc=False, verbose=False)\n",
    "    \n",
    "    # Plot the power (averaged across epochs)\n",
    "    fig = power.plot_topo(title='Time-Frequency Power (CTL Eyes-Open)', vlim=(-1, 1),\n",
    "                          colorbar=True, show=False)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pd_tremor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
